from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from sqlalchemy.orm import Session
from typing import List, Dict, Any
from datetime import datetime
import uuid
import logging

from models.schemas import (
    AnalysisRequest,
    AnalysisResult,
    AnalysisStatus,
    ItsdRecommendationRequest,
)
from analyzers.git_analyzer import GitAnalyzer
from core.database import get_db
from services.rdb_embedding_service import RDBEmbeddingService # RDBEmbeddingService ì„í¬íŠ¸
from services.embedding_service import (
    StructuredDatasetSpec,
    ITSD_DATASET_SPEC,
    get_structured_embedding_service,
)
from services.rag_analysis_service import AssigneeRecommendationService

logger = logging.getLogger(__name__)

router = APIRouter(
    prefix="/api/v1", 
    tags=["ğŸ” Git Analysis"],
    responses={
        200: {"description": "ë¶„ì„ ì„±ê³µ"},
        400: {"description": "ì˜ëª»ëœ ìš”ì²­"},
        404: {"description": "ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"},
        500: {"description": "ì„œë²„ ì˜¤ë¥˜"}
    }
)

# ë©”ëª¨ë¦¬ ìºì‹œ (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ìœ ì§€, ë°ì´í„°ë² ì´ìŠ¤ì™€ í•¨ê»˜ ì‚¬ìš©)
analysis_results = {}


DATASET_SPECS: Dict[str, StructuredDatasetSpec] = {
    "itsd": ITSD_DATASET_SPEC,
}


def _resolve_dataset(dataset_name: str) -> StructuredDatasetSpec:
    spec = DATASET_SPECS.get(dataset_name.lower())
    if spec is None:
        raise HTTPException(status_code=404, detail=f"Unknown dataset: {dataset_name}")
    return spec


def _resolve_recommendation_service(dataset_name: str) -> AssigneeRecommendationService:
    spec = _resolve_dataset(dataset_name)
    embedding_service = get_structured_embedding_service(spec)
    return AssigneeRecommendationService(embedding_service=embedding_service, dataset_spec=spec)

@router.post(
    "/analyze", 
    response_model=dict,
    summary="Git ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì‹œì‘",
    description="""
    **Git ë ˆí¬ì§€í† ë¦¬ë“¤ì„ ì‹¬ì¸µ ë¶„ì„í•˜ì—¬ ê°œë°œ ê°€ì´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.**
    
    ### ğŸ” ë¶„ì„ í•­ëª©
    - **AST ë¶„ì„**: ì½”ë“œ êµ¬ì¡°, í•¨ìˆ˜, í´ë˜ìŠ¤ ì¶”ì¶œ
    - **ê¸°ìˆ ìŠ¤í™ ë¶„ì„**: ì˜ì¡´ì„±, í”„ë ˆì„ì›Œí¬, ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°ì§€
    - **ë ˆí¬ì§€í† ë¦¬ê°„ ì—°ê´€ë„**: ê³µí†µ íŒ¨í„´, ì•„í‚¤í…ì²˜ ìœ ì‚¬ì„±
    - **ë¬¸ì„œ ìˆ˜ì§‘**: README, doc í´ë”, ì°¸ì¡° URL ìë™ ìˆ˜ì§‘
    
    ### ğŸ“ ì‚¬ìš© ì˜ˆì‹œ
    ```bash
    curl -X POST "http://localhost:8001/api/v1/analyze" \
      -H "Content-Type: application/json" \
      -d 
      '{ 
        "repositories": [
          {
            "url": "https://github.com/octocat/Hello-World.git",
            "branch": "master"
          }
        ],
        "include_ast": true,
        "include_tech_spec": true,
        "include_correlation": true,
        "group_name": "MyTeamA" # <-- ì´ ì¤„ ì¶”ê°€
      }'
    ```
    
    ### ğŸ”§ ê³ ê¸‰ ì˜µì…˜(Enhanced)
    - `include_tree_sitter`: Tree-sitter ê¸°ë°˜ AST ë¶„ì„ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ true)
    - `include_static_analysis`: ì •ì  ë¶„ì„ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ true)
    - `include_dependency_analysis`: ì˜ì¡´ì„± ë¶„ì„ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ true)
    - `generate_report`: ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì—¬ë¶€ (ê¸°ë³¸ true)

    ê¸°ì¡´ `/api/v1/enhanced/*` ì—”ë“œí¬ì¸íŠ¸ëŠ” ê³§ ì¤‘ë‹¨ ì˜ˆì •ì´ë©°, ë³¸ APIì˜ í”Œë˜ê·¸ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤.

    ### â±ï¸ ì²˜ë¦¬ ì‹œê°„
    - ì†Œê·œëª¨ ë ˆí¬ì§€í† ë¦¬: 1-3ë¶„
    - ëŒ€ê·œëª¨ ë ˆí¬ì§€í† ë¦¬: 5-15ë¶„
    - ë‹¤ì¤‘ ë ˆí¬ì§€í† ë¦¬: ë ˆí¬ì§€í† ë¦¬ ìˆ˜ì— ë¹„ë¡€
    """,
    response_description="ë¶„ì„ ì‹œì‘ í™•ì¸ ë° analysis_id ë°˜í™˜"
)
async def start_analysis(request: AnalysisRequest, background_tasks: BackgroundTasks, db: Session = Depends(get_db)):
    """Git ë ˆí¬ì§€í† ë¦¬ ë¶„ì„ ì‹œì‘ - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¹„ë™ê¸° ì²˜ë¦¬ë©ë‹ˆë‹¤."""
    try:
        from services.analysis_service import AnalysisService, RagRepositoryAnalysisService
        
        # ì¤‘ë³µ ë ˆí¬ì§€í† ë¦¬ ì²´í¬ ë° ìµœì‹  commit í™•ì¸
        from analyzers.git_analyzer import GitAnalyzer
        
        existing_analysis_ids = []
        new_repositories = []
        git_analyzer = GitAnalyzer()
        
        for repo in request.repositories:
            repo_url = repo.url
            branch = getattr(repo, 'branch', 'main')
            
            try:
                # ìµœì‹  commit ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                latest_commit_info = git_analyzer.get_latest_commit_info(repo_url, branch)
                latest_commit_hash = latest_commit_info.get('commit_hash')
                
                # ë¶„ì„ì´ í•„ìš”í•œì§€ í™•ì¸ (commit hash ë¹„êµ)
                analysis_needed, existing_analysis_id = RagRepositoryAnalysisService.check_if_analysis_needed(
                    db, repo_url, branch, latest_commit_hash
                )
                
                if analysis_needed:
                    new_repositories.append(repo)
                    logger.info(f"New analysis needed for {repo_url}:{branch} - commit: {latest_commit_hash[:8] if latest_commit_hash else 'unknown'}")
                else:
                    existing_analysis_ids.append(existing_analysis_id)
                    logger.info(f"Reusing existing analysis for {repo_url}:{branch}: {existing_analysis_id}")
                    
            except Exception as e:
                logger.warning(f"Failed to get commit info for {repo_url}: {e}")
                # commit ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
                existing_analysis_id = RagRepositoryAnalysisService.get_analysis_by_repository_url(
                    db, repo_url, branch
                )
                
                if existing_analysis_id:
                    existing_analysis_ids.append(existing_analysis_id)
                    logger.info(f"Found existing analysis for repository {repo_url}: {existing_analysis_id}")
                else:
                    new_repositories.append(repo)
        
        # ëª¨ë“  ë ˆí¬ì§€í† ë¦¬ê°€ ì´ë¯¸ ë¶„ì„ëœ ê²½ìš°, ê°€ì¥ ìµœì‹  ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        if not new_repositories and existing_analysis_ids:
            latest_analysis_id = existing_analysis_ids[0]  # ê°€ì¥ ìµœì‹  ê²ƒ ì‚¬ìš©
            logger.info(f"All repositories already analyzed. Returning latest analysis: {latest_analysis_id}")
            return {
                "analysis_id": latest_analysis_id,
                "status": "existing",
                "message": f"ëª¨ë“  ë ˆí¬ì§€í† ë¦¬ê°€ ì´ë¯¸ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {latest_analysis_id}"
            }
        
        # ìƒˆë¡œìš´ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
        analysis_id = str(uuid.uuid4())
        
        # ìƒˆë¡œìš´ ë ˆí¬ì§€í† ë¦¬ë§Œ í¬í•¨í•˜ëŠ” ìš”ì²­ ìƒì„±
        if new_repositories:
            new_request = AnalysisRequest(
                repositories=new_repositories,
                include_ast=request.include_ast,
                include_tech_spec=request.include_tech_spec,
                include_correlation=request.include_correlation,
                group_name=request.group_name # <-- ì´ ì¤„ ì¶”ê°€
            )
        else:
            new_request = request
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— AnalysisRequest ë ˆì½”ë“œ ìƒì„± (foreign key constraintë¥¼ ìœ„í•´ í•„ìš”)
        from services.analysis_service import RagAnalysisService
        try:
            # ë ˆí¬ì§€í† ë¦¬ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            repositories_data = []
            for repo in new_request.repositories:
                repositories_data.append({
                    "url": str(repo.url),
                    "branch": repo.branch or "main",
                    "name": repo.name
                })
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— AnalysisRequest ìƒì„±
            db_analysis_request = RagAnalysisService.create_analysis_request(
                db=db,
                repositories=repositories_data,
                include_ast=new_request.include_ast,
                include_tech_spec=new_request.include_tech_spec,
                include_correlation=new_request.include_correlation,
                analysis_id=analysis_id,
                group_name=new_request.group_name # <-- ì´ ì¤„ ì¶”ê°€
            )
            logger.info(f"Created AnalysisRequest in database: {analysis_id}")
            
        except Exception as e:
            logger.error(f"Failed to create AnalysisRequest in database: {e}")
            raise HTTPException(status_code=500, detail=f"ë°ì´í„°ë² ì´ìŠ¤ì— ë¶„ì„ ìš”ì²­ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        
        # ë¶„ì„ ê²°ê³¼ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ìºì‹œìš©)
        analysis_result = AnalysisResult(
            analysis_id=analysis_id,
            status=AnalysisStatus.PENDING,
            created_at=datetime.now(),
            repositories=[],
            correlation_analysis=None,
            source_summaries_used=False,
            group_name=request.group_name # <-- ì´ ì¤„ ì¶”ê°€
        )
        
        # ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
        analysis_results[analysis_id] = analysis_result
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰ (ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ë„ ì „ë‹¬)
        analysis_service = AnalysisService()
        background_tasks.add_task(analysis_service.perform_analysis, analysis_id, new_request, analysis_results, db)
        
        message = "ë¶„ì„ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        if existing_analysis_ids:
            message += f" ì¼ë¶€ ë ˆí¬ì§€í† ë¦¬ëŠ” ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤."
        
        return {
            "analysis_id": analysis_id,
            "status": "started",
            "message": f"{message} /results/{analysis_id} ì—”ë“œí¬ì¸íŠ¸ë¡œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
            "existing_analyses": existing_analysis_ids if existing_analysis_ids else None
        }
    except Exception as e:
        logger.error(f"Failed to start analysis: {e}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post(
    "/ingest_rdb_schema",
    summary="RDB ìŠ¤í‚¤ë§ˆ ì •ë³´ ì„ë² ë”©",
    description="""
    **MariaDBì˜ ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì„ë² ë”©í•©ë‹ˆë‹¤.**
    ì´ë¥¼ í†µí•´ RDB êµ¬ì¡°ì— ëŒ€í•œ ì§ˆë¬¸ì— RAG ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """,
    response_description="RDB ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ê²°ê³¼"
)
async def ingest_rdb_schema():
    """RDB ìŠ¤í‚¤ë§ˆ ì •ë³´ ì„ë² ë”© - MariaDBì˜ í…Œì´ë¸” ë° ì»¬ëŸ¼ ì •ë³´ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤."""
    try:
        rdb_embedding_service = RDBEmbeddingService()
        result = rdb_embedding_service.extract_and_embed_schema()
        return result
    except Exception as e:
        logger.error(f"Failed to ingest RDB schema: {e}")
        raise HTTPException(status_code=500, detail=f"RDB ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/results/{analysis_id}", response_model=AnalysisResult)
async def get_analysis_result(analysis_id: str, db: Session = Depends(get_db)):
    """ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        from services.analysis_service import AnalysisService
        
        analysis_service = AnalysisService()
        
        # ë¨¼ì € ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ í™•ì¸
        if analysis_id in analysis_results:
            return analysis_results[analysis_id]
        
        # ë©”ëª¨ë¦¬ì— ì—†ìœ¼ë©´ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ ì‹œë„
        result = analysis_service.load_analysis_result_from_db(analysis_id, db)
        if result:
            analysis_results[analysis_id] = result  # ë©”ëª¨ë¦¬ì— ìºì‹œ
            return result
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ë„ ì—†ìœ¼ë©´ ë””ìŠ¤í¬ì—ì„œ ë¡œë“œ ì‹œë„ (ë°±ì›Œë“œ í˜¸í™˜ì„±)
        result = analysis_service.load_analysis_result(analysis_id)
        if result:
            analysis_results[analysis_id] = result  # ë©”ëª¨ë¦¬ì— ìºì‹œ
            return result
        
        # ëª¨ë“  ê³³ì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ 404 ì—ëŸ¬
        available_ids = list(analysis_results.keys())
        error_detail = {
            "message": "ë¶„ì„ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "analysis_id": analysis_id,
            "available_analysis_ids": available_ids[:5],  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            "total_available": len(available_ids),
            "suggestions": [
                "1. ì˜¬ë°”ë¥¸ analysis_idë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.",
                "2. /results ì—”ë“œí¬ì¸íŠ¸ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì„ ê²°ê³¼ ëª©ë¡ì„ í™•ì¸í•˜ì„¸ìš”.",
                "3. ë¶„ì„ì´ ì•„ì§ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ì‹¤íŒ¨í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "4. ë¶„ì„ ID í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš” (UUID í˜•ì‹)."
            ]
        }
        raise HTTPException(status_code=404, detail=error_detail)
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        logger.error(f"Failed to get analysis result for {analysis_id}: {e}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.get("/results", response_model=List[dict])
async def list_analysis_results(db: Session = Depends(get_db)):
    """ëª¨ë“  ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        from services.analysis_result_service import AnalysisResultService
        
        analysis_result_service = AnalysisResultService()
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëª¨ë“  ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        db_results = analysis_result_service.get_all_analysis_results_from_db(db)
        
        # ë©”ëª¨ë¦¬ ìºì‹œì˜ ê²°ê³¼ì™€ ë³‘í•©
        all_results = {}
        
        # ë°ì´í„°ë² ì´ìŠ¤ ê²°ê³¼ ì¶”ê°€
        for result in db_results:
            all_results[result.analysis_id] = {
                "analysis_id": result.analysis_id,
                "status": result.status,
                "created_at": result.analysis_date,
                "completed_at": result.completed_at,
                "repository_count": result.repository_count,
                "source": "database"
            }
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ê²°ê³¼ ì¶”ê°€/ì—…ë°ì´íŠ¸
        for result in analysis_results.values():
            all_results[result.analysis_id] = {
                "analysis_id": result.analysis_id,
                "status": result.status,
                "created_at": result.created_at,
                "completed_at": result.completed_at,
                "repository_count": len(result.repositories),
                "source": "memory"
            }
        
        return list(all_results.values())
    except Exception as e:
        logger.error(f"Failed to list analysis results: {e}")
        raise HTTPException(status_code=500, detail=f"ë¶„ì„ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post(
    "/analysis/{dataset_name}/recommend-assignee",
    summary="êµ¬ì¡°í™” ë°ì´í„°ì…‹ ë‹´ë‹¹ì ì¶”ì²œ",
    response_model=str,
    tags=["Assignee Recommendation"],
)
async def recommend_assignee_for_dataset(
    dataset_name: str,
    request: ItsdRecommendationRequest,
    page: int = 1,
    page_size: int = 5,
    use_rrf: bool | None = None,
    w_title: float | None = None,
    w_content: float | None = None,
    rrf_k0: int | None = None,
    top_k_each: int | None = None,
):
    service = _resolve_recommendation_service(dataset_name)
    try:
        safe_page = max(1, int(page))
        safe_page_size = max(1, min(50, int(page_size)))
        return await service.recommend_assignee(
            title=request.title,
            description=request.description,
            page=safe_page,
            page_size=safe_page_size,
            use_rrf=use_rrf,
            w_title=w_title,
            w_content=w_content,
            rrf_k0=rrf_k0,
            top_k_each=top_k_each,
        )
    except HTTPException:
        raise
    except Exception as exc:
        logger.error("ë‹´ë‹¹ì ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (dataset=%s): %s", dataset_name, exc)
        raise HTTPException(status_code=500, detail=f"ë‹´ë‹¹ì ì¶”ì²œ ì¤‘ ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")


@router.get("/cache/stats", summary="ìºì‹œ í†µê³„ ì¡°íšŒ", description="Git ë ˆí¬ì§€í† ë¦¬ ìºì‹œ ë””ë ‰í† ë¦¬ì˜ í†µê³„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.")
async def get_cache_stats() -> Dict[str, Any]:
    """Git ë ˆí¬ì§€í† ë¦¬ ìºì‹œ í†µê³„ ì •ë³´ ì¡°íšŒ"""
    try:
        git_analyzer = GitAnalyzer()
        stats = git_analyzer.get_cache_stats()
        return stats
    except Exception as e:
        logger.error(f"Failed to get cache stats: {e}")
        raise HTTPException(status_code=500, detail=f"ìºì‹œ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


@router.post("/cache/cleanup", summary="ìºì‹œ ì •ë¦¬", description="ì˜¤ë˜ëœ Git ë ˆí¬ì§€í† ë¦¬ ìºì‹œë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.")
async def cleanup_cache(max_age_hours: int = 24) -> Dict[str, Any]:
    """ì˜¤ë˜ëœ Git ë ˆí¬ì§€í† ë¦¬ ìºì‹œ ì •ë¦¬"""
    try:
        git_analyzer = GitAnalyzer()
        cleaned_count = git_analyzer.cleanup_old_repositories(max_age_hours)
        return {
            "message": f"ìºì‹œ ì •ë¦¬ ì™„ë£Œ",
            "cleaned_repositories": cleaned_count,
            "max_age_hours": max_age_hours
        }
    except Exception as e:
        logger.error(f"Failed to cleanup cache: {e}")
        raise HTTPException(status_code=500, detail=f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
